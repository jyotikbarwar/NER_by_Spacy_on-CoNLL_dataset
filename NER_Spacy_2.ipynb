{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "c9729e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea75fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9015109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f2c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Tesla Inc is going to acquire. Twitter Inc for $44 billion ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d83602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc is going to acquire.\n",
      "Twitter Inc for $44 billion\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "#     print(token.text, \"--->\", token.pos_)\n",
    "#     print(sent.text, token.dep_, token.head.text)\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ebed25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055e4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
      "Twitter Inc | ORG | Companies, agencies, institutions, etc.\n",
      "$44 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, '|', ent.label_,\"|\", spacy.explain(ent.label_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cf3fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire. \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $44 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style = 'ent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d9b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Tesla was incorporated in July 2003 by Martin Eberhard and Marc Tarpenning as Tesla Motors. \n",
    "            The company's name is a tribute to inventor and electrical engineer Nikola Tesla. \n",
    "            In February 2004 Elon Musk joined as the company's largest shareholder and in 2008 he was named CEO. \n",
    "            In 2008, the company began production of its first car model, the Roadster sports car, \n",
    "            followed by the Model S sedan in 2012, the Model X SUV in 2015, the Model 3 sedan in 2017, \n",
    "            the Model Y crossover in 2020, the Tesla Semi truck in 2022 and the Cybertruck pickup truck in 2023. \n",
    "            The Model 3 is the all-time bestselling plug-in electric car worldwide, and in June 2021 became the \n",
    "            first electric car to sell 1 million units globally. In 2023, the Model Y was the best-selling \n",
    "            vehicle, of any kind, globally.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01182f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "document2 = \"\"\"\n",
    "            In the Himalayas, Mount Everest stands as a symbol of human achievement, \n",
    "            attracting climbers from all over the world. Last year, on January 1st, a team of ten experienced \n",
    "            mountaineers embarked on a journey to conquer the summit. They set off at 5:00 AM, braving the harsh \n",
    "            weather conditions and extreme altitudes. After weeks of arduous climbing, they reached the top, \n",
    "            achieving a remarkable feat. Their success rate was an impressive 80%, with eight out of ten climbers \n",
    "            reaching the peak. This accomplishment marked a significant milestone in mountaineering history and \n",
    "            inspired many aspiring adventurers to pursue their dreams.\n",
    "\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70faebca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>            In the \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Himalayas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mount Everest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " stands as a symbol of human achievement, <br>            attracting climbers from all over the world. \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Last year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    January 1st\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", a team of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ten\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " experienced <br>            mountaineers embarked on a journey to conquer the summit. They set off at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    5:00 AM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       ", braving the harsh <br>            weather conditions and extreme altitudes. After \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    weeks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of arduous climbing, they reached the top, <br>            achieving a remarkable feat. Their success rate was an impressive \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    80%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       ", with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    eight\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " out of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ten\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " climbers <br>            reaching the peak. This accomplishment marked a significant milestone in mountaineering history and <br>            inspired many aspiring adventurers to pursue their dreams.<br><br>            </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddd = nlp(document2)\n",
    "displacy.render(ddd, style = 'ent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a0baf",
   "metadata": {},
   "source": [
    "## NER on CoNLL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784fd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2251e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cebcae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576bf34cd9464c94b4c29acdbd983ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73eda30d8ce9437f8ce3e31a983a7ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 to /Users/jyotikumari/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965daa96184c4def810165f3f38c67f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /Users/jyotikumari/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbc144055fd4d69baf8c3901ef41a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CoNLL 2003 NER dataset\n",
    "dataset = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3880be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdda9a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32b046f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2a039964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       " [3, 0, 7, 0, 0, 0, 7, 0, 0])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ner_tuples = [(token, ner_tag) for token, ner_tag in zip(dataset[\"train\"][\"tokens\"], dataset[\"train\"][\"ner_tags\"])]\n",
    "\n",
    "token_ner_tuples[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d173759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 3),\n",
       " ('rejects', 0),\n",
       " ('German', 7),\n",
       " ('call', 0),\n",
       " ('to', 0),\n",
       " ('boycott', 0),\n",
       " ('British', 7),\n",
       " ('lamb', 0),\n",
       " ('.', 0)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_token_ner_tuples(data):\n",
    "    \n",
    "    filtered_data = [(tokens, ner_tags) for tokens, ner_tags in data if len(tokens) == len(ner_tags)]\n",
    "    \n",
    "    result = [list(zip(tokens, ner_tags)) for tokens, ner_tags in filtered_data]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "output = create_token_ner_tuples(token_ner_tuples)\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9be1a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_mapping =  { \n",
    "\n",
    " 0: 'O', \n",
    " 1: 'B-PER', \n",
    " 2: 'I-PER', \n",
    " 3:  'B-ORG', \n",
    " 4: 'I-ORG', \n",
    " 5: 'B-LOC', \n",
    " 6: 'I-LOC', \n",
    " 7: 'B-MISC', \n",
    " 8: 'I-MISC', \n",
    " 9: 'PAD', \n",
    " 10: 'UNK'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59580b",
   "metadata": {},
   "source": [
    "### inference using spacy pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "784dd7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wednesday DATE Absolute or relative dates or periods\n",
      "U.S. GPE Countries, cities, states\n",
      "first ORDINAL \"first\", \"second\", etc.\n",
      "third ORDINAL \"first\", \"second\", etc.\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join([x[0] for x in output[1111]])\n",
    "#print(text)\n",
    "#print(conll_data[1111])\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a42e45ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('EU', 3),\n",
       "  ('rejects', 0),\n",
       "  ('German', 7),\n",
       "  ('call', 0),\n",
       "  ('to', 0),\n",
       "  ('boycott', 0),\n",
       "  ('British', 7),\n",
       "  ('lamb', 0),\n",
       "  ('.', 0)],\n",
       " [('Peter', 1), ('Blackburn', 2)]]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a85066",
   "metadata": {},
   "source": [
    "# custom trained on CoNLL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "eab57c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU rejects German call to boycott British lamb .'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[\" \".join(x) for x in dataset['train'][0:][\"tokens\"]]\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d45bedbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2, 3), (11, 17, 7), (34, 41, 7)], [(0, 5, 1), (6, 15, 2)]]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_char_numbering_and_tag(tokens_with_tags):\n",
    "    # Initialize variables to keep track of character indices\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "\n",
    "    # Initialize an empty list to store the (start, end, tag) tuples\n",
    "    extracted_data = []\n",
    "\n",
    "    # Iterate through the list of tuples\n",
    "    for token, tag in tokens_with_tags:\n",
    "        # Calculate the end index for the current token\n",
    "        end_index = start_index + len(token)\n",
    "\n",
    "        # Append the (start, end, tag) tuple to the list if tag is non-zero\n",
    "        if tag != 0:\n",
    "            extracted_data.append((start_index, end_index, tag))\n",
    "\n",
    "        # Update the start index for the next token (including space)\n",
    "        start_index = end_index + 1\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "final = []\n",
    "\n",
    "for tokens_with_tags in output:\n",
    "    \n",
    "    extracted_data = extract_char_numbering_and_tag(tokens_with_tags)\n",
    "    \n",
    "    final.append(extracted_data)\n",
    "    \n",
    "final[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "cf12b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2, 'B-ORG'), (11, 17, 'B-MISC'), (34, 41, 'B-MISC')],\n",
       " [(0, 5, 'B-PER'), (6, 15, 'I-PER')],\n",
       " [(0, 8, 'B-LOC')],\n",
       " [(4, 12, 'B-ORG'),\n",
       "  (13, 23, 'I-ORG'),\n",
       "  (59, 65, 'B-MISC'),\n",
       "  (94, 101, 'B-MISC')]]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[(start, end, ner_mapping[tag]) for start, end, tag in sublist] for sublist in final]\n",
    "\n",
    "y[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a84de2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "371b5798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "2ad038da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner']\n"
     ]
    }
   ],
   "source": [
    "ner = nlp.add_pipe('ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "59683872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'B-MISC', 'B-LOC', 'B-PER', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "non_empty_y = [l for l in y_train if len(l)!= 0]\n",
    "\n",
    "for annotations in non_empty_y:\n",
    "    for entity in annotations:\n",
    "        #print(entity)\n",
    "        unique_labels.add(entity[2])\n",
    "        \n",
    "unique_labels = list(unique_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "72874e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER')"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in unique_labels:\n",
    "    ner.add_label(label)\n",
    "    \n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "73cc7381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store Example objects\n",
    "train_data = []\n",
    "\n",
    "# Iterate over each pair of text and annotations\n",
    "for text, annotations in zip(X_train, y_train):\n",
    "    # Create Doc object using make_doc method\n",
    "    doc = nlp.make_doc(text)\n",
    "    \n",
    "    # Create Example object from Doc and annotations\n",
    "    example = Example.from_dict(doc, {\"entities\": annotations})\n",
    "    \n",
    "    # Append Example object to the list\n",
    "    train_data.append(example)\n",
    "\n",
    "# Print the list of Example objects\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "99882317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_annotation': {'cats': {}, 'entities': ['U-B-LOC', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['PARIS', '1996', '-', '08', '-', '24'], 'SPACY': [True, False, False, False, False, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8142aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 13269.067364351467}\n",
      "Losses: {'ner': 7228.719209355482}\n",
      "Losses: {'ner': 5218.623908071141}\n",
      "Losses: {'ner': 4235.147827521933}\n",
      "Losses: {'ner': 3527.334686539278}\n",
      "Losses: {'ner': 3169.6016198390485}\n",
      "Losses: {'ner': 2772.917481338202}\n",
      "Losses: {'ner': 2834.981582080788}\n",
      "Losses: {'ner': 2514.9781875798863}\n",
      "Losses: {'ner': 2346.2974665604615}\n",
      "Losses: {'ner': 2122.8133694250478}\n",
      "Losses: {'ner': 2203.4749957547197}\n",
      "Losses: {'ner': 2203.4958050560695}\n",
      "Losses: {'ner': 2070.3356065093394}\n",
      "Losses: {'ner': 2076.971916894827}\n",
      "Losses: {'ner': 2045.9435269179116}\n",
      "Losses: {'ner': 1917.7357531976222}\n",
      "Losses: {'ner': 1902.9205073020244}\n",
      "Losses: {'ner': 1869.0906706415199}\n",
      "Losses: {'ner': 1837.96275765335}\n",
      "36993.486268758774\n"
     ]
    }
   ],
   "source": [
    "# Train the NER model\n",
    "import time\n",
    "start= time.time()\n",
    "import random\n",
    "\n",
    "nlp.begin_training()\n",
    "for itn in range(20):  # Adjust the number of iterations as needed\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    for example in train_data:\n",
    "        nlp.update([example], losses=losses)\n",
    "\n",
    "    print(\"Losses:\", losses)\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "22e84fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to disk\n",
    "nlp.to_disk(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "b641629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the model from disk and test it\n",
    "nlp_loaded = spacy.load(\"ner_model\")\n",
    "test_text = \"Apple is hiring new engineers for its New York office.\"\n",
    "doc = nlp_loaded(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "fdc34be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 1 B-ORG\n",
      "New 7 8 B-LOC\n",
      "York 8 9 I-LOC\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "48af2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained NER model\n",
    "nlp_loaded = spacy.load(\"ner_model\")\n",
    "\n",
    "# Test data\n",
    "test_data = [\n",
    "    (\"Apple is hiring new engineers for its New York office.\", {\"entities\": [(0, 5, \"ORG\"), (44, 53, \"LOC\")]}),\n",
    "    # Add more test examples as needed\n",
    "]\n",
    "\n",
    "# Initialize counters for evaluation metrics\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "# Iterate over the test data\n",
    "for text, annotations in zip(X_test, y_test):\n",
    "    # Process the text with the loaded NER model\n",
    "    doc = nlp_loaded(text)\n",
    "    predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    true_entities = [(text[start:end], label) for start, end, label in annotations]\n",
    "\n",
    "    # Compare predicted entities with true entities and update evaluation metrics\n",
    "    for entity in predicted_entities:\n",
    "        if entity in true_entities:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    for entity in true_entities:\n",
    "        if entity not in predicted_entities:\n",
    "            false_negatives += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "486dda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8774243599689682\n",
      "Recall: 0.8619112940100594\n",
      "F1 Score: 0.8695986467784099\n"
     ]
    }
   ],
   "source": [
    "# # Calculate precision, recall, and F1 score\n",
    "# precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "# recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "# f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "f4285d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8942731277533039\n",
      "Recall: 0.888737922705314\n",
      "F1 Score: 0.8914969334443856\n"
     ]
    }
   ],
   "source": [
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3977e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212c856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2eddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2c130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
